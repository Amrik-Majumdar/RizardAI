{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alzheimer's Disease Stage Classification from Brain MRI Scans\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook presents a deep learning framework for classifying brain MRI scans into three clinical stages: Cognitively Normal (CN), Mild Cognitive Impairment (MCI), and Alzheimer's Disease (AD). The system employs transfer learning with EfficientNet-B3, augmented with attention mechanisms and uncertainty quantification. The model serves as a clinical decision-support tool to assist radiologists in staging cognitive decline, not as a standalone diagnostic system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Background\n",
    "\n",
    "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline and brain atrophy. The disease progression follows a continuum from Cognitively Normal (CN) through Mild Cognitive Impairment (MCI) to full Alzheimer's Disease (AD).\n",
    "\n",
    "Structural MRI reveals characteristic patterns:\n",
    "- **CN**: Normal brain volume and structure\n",
    "- **MCI**: Early hippocampal atrophy, mild cortical thinning\n",
    "- **AD**: Significant hippocampal and entorhinal cortex atrophy, ventricular enlargement, widespread cortical thinning\n",
    "\n",
    "Automated classification can assist clinicians by providing quantitative assessments and highlighting regions of interest, but requires careful validation and clinical oversight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The dataset consists of brain MRI scans from Kaggle, organized into training and test sets with four impairment levels:\n",
    "- `No Impairment` → mapped to **CN** (Cognitively Normal)\n",
    "- `Very Mild Impairment` → mapped to **MCI** (Mild Cognitive Impairment)\n",
    "- `Mild Impairment` and `Moderate Impairment` → mapped to **AD** (Alzheimer's Disease)\n",
    "\n",
    "Images are provided as PNG/JPG slices in axial plane. The dataset exhibits class imbalance, which will be addressed through weighted sampling and focal loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_structure(data_dir):\n",
    "    data_path = Path(data_dir)\n",
    "    train_path = data_path / 'train'\n",
    "    test_path = data_path / 'test'\n",
    "    \n",
    "    if not train_path.exists() or not test_path.exists():\n",
    "        raise ValueError(f'Expected train/ and test/ directories in {data_dir}')\n",
    "    \n",
    "    expected_classes = ['No Impairment', 'Very Mild Impairment', 'Mild Impairment', 'Moderate Impairment']\n",
    "    \n",
    "    for split_path in [train_path, test_path]:\n",
    "        found_classes = [d.name for d in split_path.iterdir() if d.is_dir()]\n",
    "        for exp_class in expected_classes:\n",
    "            if exp_class not in found_classes:\n",
    "                raise ValueError(f'Missing class directory: {exp_class} in {split_path}')\n",
    "    \n",
    "    print('Dataset structure verified')\n",
    "    return train_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(source_dir, target_dir, target_size=(224, 224)):\n",
    "    source_path = Path(source_dir)\n",
    "    target_path = Path(target_dir)\n",
    "    \n",
    "    class_mapping = {\n",
    "        'No Impairment': 'CN',\n",
    "        'Very Mild Impairment': 'MCI',\n",
    "        'Mild Impairment': 'AD',\n",
    "        'Moderate Impairment': 'AD'\n",
    "    }\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        split_source = source_path / split\n",
    "        split_target = target_path / split\n",
    "        \n",
    "        for old_class, new_class in class_mapping.items():\n",
    "            old_class_dir = split_source / old_class\n",
    "            new_class_dir = split_target / new_class\n",
    "            new_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            image_files = list(old_class_dir.glob('*.png')) + list(old_class_dir.glob('*.jpg')) + list(old_class_dir.glob('*.jpeg'))\n",
    "            \n",
    "            for img_file in tqdm(image_files, desc=f'Processing {split}/{old_class}'):\n",
    "                try:\n",
    "                    img = Image.open(img_file)\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "                    \n",
    "                    new_filename = f'{new_class}_{img_file.stem}.png'\n",
    "                    img.save(new_class_dir / new_filename, 'PNG')\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    print('Image preprocessing completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, test_path = verify_structure('./data')\n",
    "\n",
    "if Path('data_processed').exists():\n",
    "    shutil.rmtree('data_processed')\n",
    "\n",
    "process_images('./data', 'data_processed', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, split='train'):\n",
    "        self.data_dir = Path(data_dir) / split\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        \n",
    "        class_map = {'CN': 0, 'MCI': 1, 'AD': 2}\n",
    "        \n",
    "        for class_name, label in class_map.items():\n",
    "            class_dir = self.data_dir / class_name\n",
    "            if not class_dir.exists():\n",
    "                continue\n",
    "            \n",
    "            for img_file in class_dir.glob('*.png'):\n",
    "                self.samples.append(str(img_file))\n",
    "                self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.samples[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        if len(features.shape) == 2:\n",
    "            features = features.unsqueeze(0)\n",
    "        attention_weights = self.attention(features)\n",
    "        weighted_features = features * attention_weights\n",
    "        pooled = weighted_features.sum(dim=1)\n",
    "        return pooled, attention_weights.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlzheimerClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3, dropout_rate=0.5, use_attention=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        backbone = efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*list(backbone.features.children()))\n",
    "        \n",
    "        self.feature_dim = 1536\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        if use_attention:\n",
    "            self.attention_pool = AttentionPooling(self.feature_dim)\n",
    "        else:\n",
    "            self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_attention=False):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            b, c, h, w = features.shape\n",
    "            features_flat = features.view(b, c, h * w).permute(0, 2, 1)\n",
    "            pooled, attention_weights = self.attention_pool(features_flat)\n",
    "        else:\n",
    "            pooled = self.global_pool(features).view(features.size(0), -1)\n",
    "            attention_weights = None\n",
    "        \n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, attention_weights, features\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc='Training', leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(dataloader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Validating', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    \n",
    "    all_probs = np.array(all_probs)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')\n",
    "    \n",
    "    return running_loss / len(dataloader), accuracy, roc_auc, all_preds, all_labels, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MRIDataset('data_processed', transform=train_transform, split='train')\n",
    "test_dataset = MRIDataset('data_processed', transform=val_transform, split='test')\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    np.arange(len(train_dataset)),\n",
    "    test_size=0.2,\n",
    "    stratify=[train_dataset[i][1] for i in range(len(train_dataset))],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "val_subset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "train_labels = [train_dataset[i][1] for i in train_indices]\n",
    "labels_array = np.array(train_labels, dtype=np.int64)\n",
    "unique_classes = np.unique(labels_array)\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=labels_array)\n",
    "class_weights_dict = dict(zip(unique_classes, class_weights))\n",
    "class_weights_tensor = torch.FloatTensor([class_weights_dict[i] for i in range(3)]).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f'Train samples: {len(train_subset)}, Val samples: {len(val_subset)}, Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlzheimerClassifier(num_classes=3, dropout_rate=0.5, use_attention=True).to(device)\n",
    "\n",
    "criterion = FocalLoss(alpha=class_weights_tensor, gamma=2.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "best_roc_auc = 0.0\n",
    "patience_counter = 0\n",
    "max_patience = 15\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "val_rocs = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, val_roc, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    val_rocs.append(val_roc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%, Val ROC-AUC={val_roc:.4f}')\n",
    "    \n",
    "    if val_roc > best_roc_auc:\n",
    "        best_roc_auc = val_roc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= max_patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(val_accs, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_roc, test_preds, test_labels, test_probs = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "print(f'Test Macro ROC-AUC: {test_roc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "class_names = ['CN', 'MCI', 'AD']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    y_true_binary = (np.array(test_labels) == i).astype(int)\n",
    "    y_score = test_probs[:, i]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
    "    roc_auc = roc_auc_score(y_true_binary, y_score)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves per Class', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Tradeoffs\n",
    "\n",
    "The confusion matrix reveals important clinical considerations:\n",
    "\n",
    "1. **CN vs MCI distinction**: Early-stage MCI may be misclassified as CN due to subtle structural changes. This is clinically acceptable as MCI diagnosis often requires longitudinal follow-up.\n",
    "\n",
    "2. **MCI vs AD boundary**: The model may struggle with the MCI-AD transition, which reflects the continuum nature of disease progression. False positives in AD classification require careful clinical correlation.\n",
    "\n",
    "3. **Sensitivity vs Specificity**: High sensitivity for AD detection is critical for early intervention, but must be balanced against false positives that could cause unnecessary patient anxiety.\n",
    "\n",
    "4. **Class imbalance impact**: The model's performance on minority classes (typically MCI) may be lower, necessitating careful interpretation in clinical settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "        \n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "        \n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        self.model.eval()\n",
    "        input_tensor = input_tensor.unsqueeze(0).to(device) if len(input_tensor.shape) == 3 else input_tensor.to(device)\n",
    "        \n",
    "        output = self.model(input_tensor)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        output[0, class_idx].backward()\n",
    "        \n",
    "        gradients = self.gradients[0]\n",
    "        activations = self.activations[0]\n",
    "        \n",
    "        weights = torch.mean(gradients, dim=(1, 2), keepdim=True)\n",
    "        cam = torch.sum(weights * activations, dim=0)\n",
    "        cam = F.relu(cam)\n",
    "        cam = F.interpolate(cam.unsqueeze(0).unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        \n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = model.backbone[-1]\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, (image, label) in enumerate(test_dataset):\n",
    "    if idx >= 3:\n",
    "        break\n",
    "    \n",
    "    image_tensor = val_transform(image).to(device)\n",
    "    output = model(image_tensor.unsqueeze(0))\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    prob = F.softmax(output, dim=1)[0, pred_class].item()\n",
    "    \n",
    "    cam = gradcam.generate(image_tensor, pred_class)\n",
    "    \n",
    "    img_array = np.array(image)\n",
    "    if len(img_array.shape) == 3 and img_array.shape[2] == 3:\n",
    "        img_display = img_array\n",
    "    else:\n",
    "        img_display = np.stack([img_array] * 3, axis=-1) if len(img_array.shape) == 2 else img_array\n",
    "    \n",
    "    axes[idx, 0].imshow(img_display)\n",
    "    axes[idx, 0].set_title(f'Original\\nTrue: {class_names[label]}')\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    axes[idx, 1].imshow(cam, cmap='jet')\n",
    "    axes[idx, 1].set_title('Grad-CAM Heatmap')\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    overlay = img_display.copy() / 255.0 if img_display.max() > 1 else img_display.copy()\n",
    "    cam_resized = np.stack([cam] * 3, axis=-1)\n",
    "    overlay = 0.6 * overlay + 0.4 * cam_resized\n",
    "    \n",
    "    axes[idx, 2].imshow(overlay)\n",
    "    axes[idx, 2].set_title(f'Overlay\\nPred: {class_names[pred_class]} ({prob:.2f})')\n",
    "    axes[idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Relevance of Saliency Maps\n",
    "\n",
    "Grad-CAM visualizations highlight regions the model uses for classification decisions. In Alzheimer's Disease:\n",
    "\n",
    "1. **Hippocampal regions**: Expected high activation in medial temporal lobe structures, consistent with known AD pathology.\n",
    "\n",
    "2. **Ventricular enlargement**: Attention to lateral ventricles aligns with disease progression markers.\n",
    "\n",
    "3. **Cortical regions**: Activation in parietal and frontal cortices may reflect cortical thinning patterns.\n",
    "\n",
    "4. **Validation**: Overlap with known neuroanatomical AD markers provides face validity, though clinical correlation remains essential.\n",
    "\n",
    "These maps assist radiologists by directing attention to regions of interest, but should not replace comprehensive image review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics & Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Bias\n",
    "\n",
    "The model is trained on a specific dataset that may not represent global population diversity. Potential biases include:\n",
    "\n",
    "- **Demographic bias**: Age, sex, ethnicity, and socioeconomic factors may not be representative\n",
    "- **Geographic bias**: Data from specific regions or healthcare systems\n",
    "- **Scanner bias**: MRI acquisition protocols, field strengths, and manufacturers vary\n",
    "- **Selection bias**: Inclusion/exclusion criteria may favor certain patient populations\n",
    "\n",
    "### Scanner Variability\n",
    "\n",
    "MRI scans vary significantly across:\n",
    "\n",
    "- Field strength (1.5T vs 3T)\n",
    "- Acquisition protocols (slice thickness, resolution, contrast)\n",
    "- Manufacturer-specific image characteristics\n",
    "- Preprocessing pipelines\n",
    "\n",
    "The model's performance may degrade when applied to scans from different scanners or protocols not represented in training data.\n",
    "\n",
    "### Generalization Limits\n",
    "\n",
    "1. **Temporal stability**: Model performance may change as imaging technology evolves\n",
    "2. **Comorbidity**: Performance may degrade with concurrent neurological conditions\n",
    "3. **Early-stage detection**: Limited sensitivity for very early MCI or pre-symptomatic AD\n",
    "4. **Population shifts**: Performance on populations with different disease prevalence\n",
    "\n",
    "### Non-Diagnostic Nature\n",
    "\n",
    "This model is a **decision-support tool**, not a diagnostic system. Key limitations:\n",
    "\n",
    "- AD diagnosis requires comprehensive clinical evaluation, not imaging alone\n",
    "- Model outputs are probabilities, not definitive diagnoses\n",
    "- False positives/negatives can have significant clinical consequences\n",
    "- Model cannot account for patient history, symptoms, or other diagnostic information\n",
    "\n",
    "### Need for Clinician Oversight\n",
    "\n",
    "1. **Mandatory review**: All model predictions must be reviewed by qualified radiologists or neurologists\n",
    "2. **Clinical context**: Model outputs must be integrated with patient history, symptoms, and other diagnostic tests\n",
    "3. **Quality assurance**: Regular monitoring of model performance in clinical deployment\n",
    "4. **Continuous validation**: Ongoing evaluation against ground truth and clinical outcomes\n",
    "5. **Regulatory compliance**: Deployment must comply with medical device regulations (FDA, CE marking, etc.)\n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "- **Patient autonomy**: Patients should be informed about AI-assisted analysis\n",
    "- **Privacy**: MRI data contains sensitive health information requiring strict privacy protection\n",
    "- **Equity**: Ensure model benefits are accessible across diverse populations\n",
    "- **Transparency**: Clinicians and patients should understand model limitations\n",
    "- **Accountability**: Clear responsibility for clinical decisions remains with healthcare providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvements\n",
    "\n",
    "1. **Multi-modal fusion**: Integrate structural MRI with functional MRI (fMRI), PET scans, or cerebrospinal fluid biomarkers\n",
    "2. **Longitudinal modeling**: Incorporate temporal information from follow-up scans to track disease progression\n",
    "3. **3D architectures**: Full volumetric analysis using 3D CNNs or vision transformers\n",
    "4. **Self-supervised pretraining**: Domain-specific pretraining on large unlabeled medical imaging datasets\n",
    "5. **Transformer architectures**: Vision transformers (ViT) or medical-specific transformer designs\n",
    "\n",
    "### Clinical Integration\n",
    "\n",
    "1. **Prospective validation**: Large-scale multi-center prospective studies\n",
    "2. **Real-world deployment**: Integration into clinical PACS systems and radiology workflows\n",
    "3. **Outcome prediction**: Extend beyond classification to predict disease progression rates\n",
    "4. **Treatment response**: Model response to therapeutic interventions\n",
    "\n",
    "### Technical Enhancements\n",
    "\n",
    "1. **Uncertainty quantification**: Bayesian neural networks or ensemble methods for reliable confidence estimates\n",
    "2. **Federated learning**: Train across institutions while preserving data privacy\n",
    "3. **Domain adaptation**: Techniques to improve generalization across scanner types and protocols\n",
    "4. **Interpretability**: Enhanced explainability methods tailored for medical imaging\n",
    "\n",
    "### Dataset Expansion\n",
    "\n",
    "1. **Diverse populations**: Inclusion of underrepresented demographic groups\n",
    "2. **Multi-center data**: Aggregation of data from diverse healthcare systems\n",
    "3. **Rare variants**: Training on atypical AD presentations and variants\n",
    "4. **Comorbidity cases**: Inclusion of patients with multiple neurological conditions\n",
    "\n",
    "### Regulatory & Validation\n",
    "\n",
    "1. **Regulatory approval**: Pursue FDA 510(k) or De Novo pathways for clinical use\n",
    "2. **Clinical trials**: Randomized controlled trials assessing impact on diagnostic accuracy and patient outcomes\n",
    "3. **Cost-effectiveness**: Health economic analyses of AI-assisted diagnosis\n",
    "4. **Standardization**: Development of standardized evaluation protocols and benchmarks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
